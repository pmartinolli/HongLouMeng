plt.tight_layout()
plt.savefig('images/similarity_matrix2.png', dpi=300, bbox_inches='tight')
plt.show()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
# Step 4: Convert to DataFrame
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
# Step 4: Convert to DataFrame
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| label: most-mentioned-characters
#| fig-cap: "Most mentioned characters"
#| code-fold: true
#| echo: false
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
# Use SimHei font for Chinese support on Windows
font_path = "C:/Windows/Fonts/simhei.ttf"
zh_font = fm.FontProperties(fname=font_path)
# Select top 20
top_20 = df_counts.head(20)
# Plot
plt.figure(figsize=(12, 8))
# bar management
bars = plt.barh(top_20["MainName"], top_20["Count"], color="skyblue")
# Add count values to the right of each bar
for bar in bars:
width = bar.get_width()
plt.text(width + 1, bar.get_y() + bar.get_height() / 2,
str(width), va='center', fontsize=10, fontproperties=zh_font)
plt.barh(top_20["MainName"], top_20["Count"], color="lightpink")
plt.xlabel("Count", fontproperties=zh_font)
plt.ylabel("Name", fontproperties=zh_font)
plt.title("Top 20 Most Mentioned Names")
plt.gca().invert_yaxis()  # So the highest is at the top
plt.tight_layout()
plt.yticks(fontproperties=zh_font)
plt.xticks(fontproperties=zh_font)
plt.show()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
for text in df_cn["RawContent"]:
#words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
# Step 4: Convert to DataFrame
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
for text in df_cn["RawContent"]:
#words = jieba.lcut(text)
words = text
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
# Step 4: Convert to DataFrame
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
# Step 4: Convert to DataFrame
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
### method1 : sequencing with jieba
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
### method2 : Count matches without double-counting shorter substrings
counts = defaultdict(int)
for text in df_cn["RawContent"]:
temp_text = text  # we will remove matched substrings as we go
for main, synonym in main_synonym_pairs:
# Escape any special regex characters
pattern = re.escape(synonym)
matches = re.findall(pattern, temp_text)
if matches:
count = len(matches)
counts[(main, synonym)] += count
# Remove matches from temp_text so they aren't double-counted
temp_text = re.sub(pattern, " ", temp_text)
rows = []
for (main, synonym), count in counts.items():
rows.append({"MainName": main, "Synonym": synonym, "Count": count})
df_counts = pd.DataFrame(rows).sort_values(by=["MainName", "Count"], ascending=[True, False])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
### method1 : sequencing with jieba
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
### method2 : Count matches without double-counting shorter substrings
# Create list of (MainName, Synonym) and sort by descending synonym length
main_synonym_pairs = []
for main, synonyms in user_dict.items():
main_synonym_pairs.append((main, main))
for synonym in synonyms:
main_synonym_pairs.append((main, synonym))
# Sort synonyms by length (longest first)
main_synonym_pairs.sort(key=lambda x: len(x[1]), reverse=True)
# Count matches without double-counting shorter substrings
counts = defaultdict(int)
for text in df_cn["RawContent"]:
temp_text = text  # we will remove matched substrings as we go
for main, synonym in main_synonym_pairs:
# Escape any special regex characters
pattern = re.escape(synonym)
matches = re.findall(pattern, temp_text)
if matches:
count = len(matches)
counts[(main, synonym)] += count
# Remove matches from temp_text so they aren't double-counted
temp_text = re.sub(pattern, " ", temp_text)
rows = []
for (main, synonym), count in counts.items():
rows.append({"MainName": main, "Synonym": synonym, "Count": count})
df_counts = pd.DataFrame(rows).sort_values(by=["MainName", "Count"], ascending=[True, False])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
View(df_counts)
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
# Step 1: Create a reverse lookup — map each synonym to its main name
synonym_to_main = {}
for main, synonyms in user_dict.items():
synonym_to_main[main] = main  # main name maps to itself
for synonym in synonyms:
synonym_to_main[synonym] = main
# Step 2: Initialize counter
counts = defaultdict(int)
# Step 3: Count occurrences across df_cn["RawContent"]
### method1 : sequencing with jieba
for text in df_cn["RawContent"]:
words = jieba.lcut(text)
for word in words:
if word in synonym_to_main:
main_name = synonym_to_main[word]
counts[main_name] += 1
df_counts = pd.DataFrame([
{"MainName": name, "Count": count} for name, count in counts.items()
])
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
#| code-fold: true
# build a df based on the userdict.json and count how many times each key (and their synonyms) are mentioned in df_cn["RawContent"] column
from collections import defaultdict
### method2 : Count matches without double-counting shorter substrings
# Create list of (MainName, Synonym) and sort by descending synonym length
main_synonym_pairs = []
for main, synonyms in user_dict.items():
main_synonym_pairs.append((main, main))
for synonym in synonyms:
main_synonym_pairs.append((main, synonym))
# Sort synonyms by length (longest first)
main_synonym_pairs.sort(key=lambda x: len(x[1]), reverse=True)
# Count matches without double-counting shorter substrings
counts = defaultdict(int)
for text in df_cn["RawContent"]:
temp_text = text  # we will remove matched substrings as we go
for main, synonym in main_synonym_pairs:
# Escape any special regex characters
pattern = re.escape(synonym)
matches = re.findall(pattern, temp_text)
if matches:
count = len(matches)
counts[(main, synonym)] += count
# Remove matches from temp_text so they aren't double-counted
temp_text = re.sub(pattern, " ", temp_text)
rows = []
for (main, synonym), count in counts.items():
rows.append({"MainName": main, "Synonym": synonym, "Count": count})
df_counts = pd.DataFrame(rows).sort_values(by=["MainName", "Count"], ascending=[True, False])
## export the result to control
#with open('temp/output.txt', 'w', encoding='utf-8') as f:
#    for _, row in df_cn.iterrows():
#        f.write(f"\n\n{row['Chapter_num']} // {row['Chapter']}\n\n {row['RawContent']}\n")
# Optional: sort by frequency
df_counts = df_counts.sort_values(by="Count", ascending=False).reset_index(drop=True)
df_counts.head()
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# Collect all nested keys across all IDs
nested_keys = set()
for id_key, nested_dict in data.items():
nested_keys.update(nested_dict.keys())
# Now nested_keys is a set of all keywords you want to search for
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
import pandas as pd
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
content_lower = content.lower()
for key in nested_keys:
if key.lower() in content_lower:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
import pandas as pd
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
content_lower = content.lower()
for key in nested_keys:
if key.lower() in content_lower:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
df_counts.head()
View(nested_keys)
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
content_lower = content.lower()
for key in nested_keys:
if key.lower() in content_lower:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
counts_df.head()
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
content_lower = content.lower()
for key in nested_keys:
if key.lower() in content_lower:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
counts_df = counts_df.sort_values(by="Count", ascending=False).reset_index(drop=True)
counts_df.head()
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
#content_lower = content.lower()
for key in nested_keys:
if key.lower() in content_lower:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
counts_df = counts_df.sort_values(by="Count", ascending=False).reset_index(drop=True)
counts_df.head()
# Read a JSON file (toto.json) — where the first keys are IDs (strings), and the nested keys are the keywords (also strings).
# Count how often each nested key appears in the RawContent column of a Pandas DataFrame (df_cn)
import json
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
## Extract all nested keys
# data is a dict where each value is a list of strings
nested_keys = set()
for id_key, list_of_keywords in data.items():
nested_keys.update(list_of_keywords)
# Now nested_keys is something like: {'toto', 'titi lulu', 'ramiou', 'volok'}
key_counts = {key: 0 for key in nested_keys}
for content in df_cn['RawContent'].dropna():
#content_lower = content.lower()
for key in nested_keys:
if key.lower() in content:
key_counts[key] += 1
counts_df = pd.DataFrame.from_dict(key_counts, orient='index', columns=['Count']).reset_index()
counts_df = counts_df.rename(columns={'index': 'Keyword'})
counts_df = counts_df.sort_values(by="Count", ascending=False).reset_index(drop=True)
counts_df.head()
import json
import pandas as pd
# 1. Load the JSON
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
# 2. Extract all values into a list
rows = []
for id_key, list_of_keywords in data.items():
for keyword in list_of_keywords:
rows.append({'Name': keyword, 'NamePattern': keyword})
# 3. Create a DataFrame
df_keywords = pd.DataFrame(rows)
print(df_keywords)
import json
import pandas as pd
# 1. Load the JSON
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
# 2. Extract all values into a list
rows = []
for id_key, list_of_keywords in data.items():
for keyword in list_of_keywords:
rows.append({'Name': id_key, 'NamePattern': keyword})
# 3. Create a DataFrame
df_keywords = pd.DataFrame(rows)
print(df_keywords)
import json
import pandas as pd
# 1. Load the JSON
with open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:
data = json.load(f)
# 2. Extract all values into a list
rows = []
for id_key, list_of_keywords in data.items():
for keyword in list_of_keywords:
rows.append({'Name': id_key, 'NamePattern': keyword})
# 3. Create a DataFrame
df_keywords = pd.DataFrame(rows)
# print(df_keywords)
# Now count
# Initialize a new column for counts
df_keywords['Count'] = 0
# Loop through each NamePattern
for idx, row in df_keywords.iterrows():
pattern = row['NamePattern'].lower()  # Lowercase for case-insensitive matching
count = df_cn['RawContent'].dropna().str.lower().str.count(pattern).sum()
df_keywords.at[idx, 'Count'] = count
print(df_keywords)
