[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "",
    "text": "This paper presents data visualizations focusing on the three main protagonists of Dream of the Red Chamber (红楼梦), a classic work of Chinese literature. It presents the results of some experiments I conducted using Python in a Quarto environment with Jupyter."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "",
    "text": "This paper presents data visualizations focusing on the three main protagonists of Dream of the Red Chamber (红楼梦), a classic work of Chinese literature. It presents the results of some experiments I conducted using Python in a Quarto environment with Jupyter."
  },
  {
    "objectID": "index.html#importing-data",
    "href": "index.html#importing-data",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "Importing data",
    "text": "Importing data\nImporting fulltext_simplified.txt, a text file retrieved from lilesIII. It is written with simplified chinese characters.\n\n\nCode\n# The echo: false option disables the printing of code (only output is displayed).\n\nwith open(\"data/fulltext_simplified.txt\", \"r\", encoding=\"utf-8\") as file:\n    main_text = file.read()\n\n\nPutting the content into a panda dataframe and shampooing the data :\n\n\n\n\n\n\n\n\nChapter_num\nChapter\nRawContent\n\n\n\n\ninteger\nextracted from header of the chapter\nfull text of the chapter"
  },
  {
    "objectID": "index.html#how-many-times-the-3-main-protogonists-are-mentioned",
    "href": "index.html#how-many-times-the-3-main-protogonists-are-mentioned",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "How many times the 3 main protogonists are mentioned",
    "text": "How many times the 3 main protogonists are mentioned\n\nCounting\nWe construct a list of name variants and synonyms for the three main protagonists—Jia Baoyu, Lin Daiyu, and Xue Baochai—and quantify their occurrences throughout the 120 chapters.\n\n\nCode\n# Count how many times the three main protagonists are counted in RawContent\n# Synonyms are enriched thanks to Wikidata (in labels).\n\n# List of Jia Baoyu's synonyms https://www.wikidata.org/wiki/Q8428650\njia_baoyu_synonyms = [\n    '贾宝玉', '賈寶玉', '寶玉', '寶二爺', '怡紅公子',\n    '絳洞花王', '富貴閒人', '宝玉', '宝二爷', \n    '絳洞花主', '绛洞花主', '怡红公子', '绛洞花王'\n]\n\n# Join them into a regex pattern\njia_baoyu_pattern = '|'.join(jia_baoyu_synonyms)\n\n# Use in your count\ndf_cn['count_JiaBaoyu'] = df_cn['RawContent'].str.count(jia_baoyu_pattern)\n\n\n\n\n# List of Lin Daiyu's synonyms\nlin_daiyu_synonyms = [\n    '林黛玉', '黛玉', '瀟湘妃子', '顰兒', '颦儿',\n    '林姑娘', '林妹妹', '潇湘妃子', \"玉儿\", \"颦颦\",\n]\n\n# Join them into a regex pattern\nlin_daiyu_pattern = '|'.join(lin_daiyu_synonyms)\n\n# Use in your count\ndf_cn['count_LinDaiyu'] = df_cn['RawContent'].str.count(lin_daiyu_pattern)\n\n\n\n\n# List of Xue Baochai's synonyms\nxue_baochai_synonyms = [\n    '薛寶釵', '薛宝钗', '寶釵', '宝钗',\n    '蘅蕪君', '蘅芜君', '寶姑娘', '宝姑娘',\n    '寶丫頭', '宝丫头', '寶姐姐', '宝姐姐'\n]\n\n# Join them into a regex pattern\nxue_baochai_pattern = '|'.join(xue_baochai_synonyms)\n\n# Use in your count\ndf_cn['count_XueBaochai'] = df_cn['RawContent'].str.count(xue_baochai_pattern)\n\n\n\n\nDrawing a heatmap\nA chapter-by-chapter heatmap is generated, where color intensity corresponds to the frequency of mentions of any of the three main protagonists.\n\n\nCode\n#Note that we included the cell option fold: true to hide the code by default (click the Code button to show it).\n\n# pip install scikit-learn jieba seaborn matplotlib\n\n# Draw a heat map for each entry and the values of the columns \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set Chapter_num as index for labeling rows in the heatmap\nheatmap_data = df_cn.set_index('Chapter_num')[['count_JiaBaoyu', 'count_LinDaiyu', 'count_XueBaochai']]\n\n\n# Create the heatmap\n\n# Plot the heatmap without labels or annotations\nplt.figure(figsize=(10, 6))\nsns.heatmap(\n    heatmap_data,\n    annot=False,          # No numbers inside cells\n    cmap='RdPu',          # palette of colors\n    linewidths=0.5,\n    cbar=True,\n    xticklabels=True,    #  column names\n    yticklabels=False     # No row labels\n)\n\n# Set y-ticks at intervals of 25\nnum_rows = heatmap_data.shape[0]\nticks = np.arange(0, num_rows, 25)\nplt.yticks(ticks + 0.5, ticks + 1)  # +0.5 centers ticks in cells, +1 shifts to 1-based indexing\n\nplt.title('«Dream of the Red Chamber» Three Main Protagonists \\n(Number of Mentions per Chapter)')\nplt.ylabel('Chapter Number')\nplt.xlabel('')  # Remove the x-axis label\nplt.tight_layout()\nplt.savefig('images/characters_count_hlm.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Three Main Protagonists\n\n\n\n\n\n\n\nDrawing a stacked bars graph\nIt’s not very readable, but it gives a rough idea of which parts of the novel involve interactions between the protagonists.\n\n\nCode\n# Set up data\nchapter_nums = df_cn['Chapter_num']\ncounts_baoyu = df_cn['count_JiaBaoyu']\ncounts_daiyu = df_cn['count_LinDaiyu']\ncounts_baochai = df_cn['count_XueBaochai']\n\n# Set figure size\nplt.figure(figsize=(12, 6))\n\n# Plot stacked bars\nplt.bar(chapter_nums, counts_baoyu, label='Jia Baoyu', color='mediumvioletred')\nplt.bar(chapter_nums, counts_daiyu, bottom=counts_baoyu, label='Lin Daiyu', color='orchid')\nplt.bar(\n    chapter_nums,\n    counts_baochai,\n    bottom=counts_baoyu + counts_daiyu,\n    label='Xue Baochai',\n    color='plum'\n)\n\n# Labels and legend\nplt.title('«Dream of the Red Chamber» Three Main Protagonists\\n(Number of Mentions per Chapter)')\nplt.xlabel('Chapter Number')\nplt.ylabel('Number of Mentions')\nplt.legend()\nplt.tight_layout()\n\n# Save and show\nplt.savefig('images/characters_count_stackedbars.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Three Main Protagonists\n\n\n\n\n\n\n\nAnother bar graph\nThis one is not stacked, so it’s more readable.\n\n\nCode\n# Set up data\nchapter_nums = df_cn['Chapter_num']\ncounts_baoyu = df_cn['count_JiaBaoyu']\ncounts_daiyu = df_cn['count_LinDaiyu']\ncounts_baochai = df_cn['count_XueBaochai']\n\n# Set figure and axes\nfig, axs = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n\n# Jia Baoyu\naxs[0].bar(chapter_nums, counts_baoyu, color='mediumvioletred')\naxs[0].set_title('Jia Baoyu – Number of Mentions per Chapter')\naxs[0].set_ylabel('Mentions')\n\n# Lin Daiyu\naxs[1].bar(chapter_nums, counts_daiyu, color='orchid')\naxs[1].set_title('Lin Daiyu – Number of Mentions per Chapter')\naxs[1].set_ylabel('Mentions')\n\n# Xue Baochai\naxs[2].bar(chapter_nums, counts_baochai, color='plum')\naxs[2].set_title('Xue Baochai – Number of Mentions per Chapter')\naxs[2].set_ylabel('Mentions')\naxs[2].set_xlabel('Chapter Number')\n\n# Tweak layout\nplt.suptitle('«Dream of the Red Chamber» – Character Mentions per Chapter', fontsize=16, y=1.02)\nplt.tight_layout()\nplt.savefig('images/characters_count_separate_bars.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Three Main Protagonists"
  },
  {
    "objectID": "index.html#number-of-times-each-pair-of-protagonists-is-mentioned-together",
    "href": "index.html#number-of-times-each-pair-of-protagonists-is-mentioned-together",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "Number of times each pair of protagonists is mentioned together",
    "text": "Number of times each pair of protagonists is mentioned together\n\nCounting\nThreshold (maximum distance between two names mentioned) = 20 words.\n\n\nCode\n# Count how many times there is a jia_baoyu_synonyms AND a lin_daiyu_synonyms within 20/threshold characters.\n\nthreshold = 20\n\n# Compile regex patterns (non-capturing groups for clarity)\njia_baoyu_pattern = r'(?:' + '|'.join(jia_baoyu_synonyms) + r')'\nlin_daiyu_pattern = r'(?:' + '|'.join(lin_daiyu_synonyms) + r')'\n\n# Pattern: 贾宝玉 followed by ≤20/threshold chars then 林黛玉, OR 林黛玉 followed by ≤20/threshold chars then 贾宝玉\njia_lin_pattern = rf'{jia_baoyu_pattern}.{{0,{threshold}}}{lin_daiyu_pattern}|{lin_daiyu_pattern}.{{0,{threshold}}}{jia_baoyu_pattern}'\n\n# Function to count matches in a string\ndef count_jia_lin_pairs(text):\n    return len(re.findall(jia_lin_pattern, text))\n\n# Apply to the DataFrame\ndf_cn['JiaBaoyu_LinDaiyu'] = df_cn['RawContent'].apply(count_jia_lin_pairs)\n\n\n\n# count pairs of jia et xue\n\nxue_baochai_pattern = r'(?:' + '|'.join(xue_baochai_synonyms) + r')'\n\n# Pattern: 贾宝玉 followed by ≤20/threshold chars then 林黛玉, OR 林黛玉 followed by ≤20/threshold chars then 贾宝玉\njia_xue_pattern = rf'{jia_baoyu_pattern}.{{0,{threshold}}}{xue_baochai_pattern}|{xue_baochai_pattern}.{{0,{threshold}}}{jia_baoyu_pattern}'\n\n# Function to count matches in a string\ndef count_jia_xue_pairs(text):\n    return len(re.findall(jia_xue_pattern, text))\n\n# Apply to the DataFrame\ndf_cn['JiaBaoyu_XueBaochai'] = df_cn['RawContent'].apply(count_jia_xue_pairs)\n\n\n\n# count pairs of lin et xue\n\nlin_xue_pattern = rf'{lin_daiyu_pattern}.{{0,{threshold}}}{xue_baochai_pattern}|{xue_baochai_pattern}.{{0,{threshold}}}{lin_daiyu_pattern}'\n\n# Function to count matches in a string\ndef count_lin_xue_pairs(text):\n    return len(re.findall(lin_xue_pattern, text))\n\n# Apply to the DataFrame\ndf_cn['LinDaiyu_XueBaochai'] = df_cn['RawContent'].apply(count_lin_xue_pairs)\n\n\n\n\nDrawing\n\n\nCode\n# Set Chapter_num as index for labeling rows in the heatmap\nheatmap_data = df_cn.set_index('Chapter_num')[['JiaBaoyu_LinDaiyu', 'JiaBaoyu_XueBaochai','LinDaiyu_XueBaochai']]\n\n# # Create the heatmap\n\n# Plot the heatmap without labels or annotations\nplt.figure(figsize=(10, 6))\nsns.heatmap(\n    heatmap_data,\n    annot=False,          # No numbers inside cells\n    cmap='RdPu',\n    linewidths=0.5,\n    cbar=True,\n    xticklabels=True,    #  column names\n    yticklabels=False     # No row labels\n)\n\n# Set y-ticks at intervals of 25\nnum_rows = heatmap_data.shape[0]\nticks = np.arange(0, num_rows, 25)\nplt.yticks(ticks + 0.5, ticks + 1)  # +0.5 centers ticks in cells, +1 shifts to 1-based indexing\n\nplt.title('«Dream of the Red Chamber» Pairs of Protagonists \\n Mentioned Together (per Chapter)')\nplt.ylabel('Chapter Number')\nplt.xlabel('')  # Remove the x-axis label\nplt.tight_layout()\nplt.savefig('images/pairs_characters_hlm.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Pairs of Two Protagonists"
  },
  {
    "objectID": "index.html#expanding-the-visualizations-to-other-characters-of-the-novel",
    "href": "index.html#expanding-the-visualizations-to-other-characters-of-the-novel",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "Expanding the visualizations to other characters of the novel",
    "text": "Expanding the visualizations to other characters of the novel\n\nChanging the counting method\nFrom now on, I will not use manual synonyms like before because it’s too heavy for managing a high number of characters.\nFrom lileslll, I found the file userdict.json. It contains 165 characters of the novel and theirs nicknames… I rename it userdict_simplified.json.\n\n\nFirst simple similarity matrix\nI filter the corpus by removing all words not found in userdict_simplified.json, allowing us to build a simple similarity matrix based solely on the characters mentioned in each chapter.\n\n\nCode\nimport jieba\nimport json\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Load the user dictionary\nwith open(\"data/userdict_simplified.json\", \"r\", encoding=\"utf-8\") as f:\n    user_dict = json.load(f)\n\n# Flatten all names and synonyms into a set for fast lookup\nvalid_names = set(user_dict.keys())\nfor synonyms in user_dict.values():\n    valid_names.update(synonyms)\n\nimport jieba\n\ndef extract_only_names(text):\n    words = jieba.lcut(text)\n    names = [word for word in words if word in valid_names]\n    return ' '.join(names)\n\n# Apply to create new column\ndf_cn['OnlyNames'] = df_cn['RawContent'].apply(extract_only_names)\n\n\n\n\n\n\n# TF-IDF Vectorization on segmented text\nvectorizer = TfidfVectorizer()\ntfidf_matrix = vectorizer.fit_transform(df_cn['OnlyNames'])\n\n# Cosine similarity between chapters\nsimilarity_matrix = cosine_similarity(tfidf_matrix)\n\n# Format as DataFrame for heatmap\nsimilarity_df = pd.DataFrame(\n    similarity_matrix,\n    index=[f\"{idx+1}\" for idx in range(len(df_cn))],\n    columns=[f\"{idx+1}\" for idx in range(len(df_cn))]\n)\n\n\n\n# Plot heatmap\nplt.figure(figsize=(12, 10))\nsns.heatmap(similarity_df, \n            cmap='RdPu', \n            vmin=0, \n            mask=np.eye(similarity_df.shape[0]), \n            vmax=1,\n            linewidths=0.2)\n\nplt.title(\"Chapter Similarity Matrix (based on a selection of characters)\")\nplt.xlabel(\"Chapters\", ha='right')\nplt.ylabel(\"Chapters\")\nplt.tight_layout()\nplt.savefig('images/similarity_matrix2.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n\nBuilding prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\martinop\\AppData\\Local\\Temp\\jieba.cache\nLoading model cost 0.859 seconds.\nPrefix dict has been built successfully.\n\n\n\n\n\n\n\n\nFigure 5: Similarity matrix"
  },
  {
    "objectID": "index.html#who-are-the-most-mentioned-characters",
    "href": "index.html#who-are-the-most-mentioned-characters",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "Who are the most mentioned characters ?",
    "text": "Who are the most mentioned characters ?\nPut the userdic_simplified into a dataframe with two columns : Name and NamePattern.\n\nStructure of the JSON (sample)\n{\n    \"贾宝玉\": [\n        \"贾宝玉\",\n        \"宝玉\",\n        \"宝二爷\",\n        \"怡红公子\",\n        \"绛洞花主\",\n        \"宝兄弟\"\n    ],\n    \"林黛玉\": [\n        \"林黛玉\",            \nThe first key will be Name and the second key will be NamePattern. These keys become the columns of the dataframe. A 3rd column, Count, stores the number of mentions.\n\n\n\nName\nNamePattern\nCount\n\n\n\n\n贾宝玉\n贾宝玉\n\n\n\n贾宝玉\n宝玉\n\n\n\n贾宝玉\n宝二爷\n\n\n\n\n\n\nCode\nimport json\nimport pandas as pd\n\n# 1. Load the JSON\nwith open('data/userdict_simplified.json', 'r', encoding='utf-8') as f:\n    data = json.load(f)\n\n# 2. Extract all values into a list\nrows = []\nfor id_key, list_of_keywords in data.items():\n    for keyword in list_of_keywords:\n        rows.append({'Name': id_key, 'NamePattern': keyword})\n\n# 3. Create a DataFrame\ndf_keywords = pd.DataFrame(rows)\n\n# print(df_keywords)\n\n\n# Now count\n# Initialize a new column for counts\ndf_keywords['Count'] = 0\n\n# Loop through each NamePattern\nfor idx, row in df_keywords.iterrows():\n    pattern = row['NamePattern'].lower()  # Lowercase for case-insensitive matching\n    count = df_cn['RawContent'].dropna().str.lower().str.count(pattern).sum()\n    df_keywords.at[idx, 'Count'] = count\n\n#print(df_keywords)\n# Export the result for control\ndf_keywords.to_csv('temp/count_keywords.csv', index=False)\n\n# Group by Name, then sum the counts\ntotal_per_Name = df_keywords.groupby('Name', as_index=False)['Count'].sum()\n\n# Sort by Count descending\ntotal_per_Name = total_per_Name.sort_values(by='Count', ascending=False)\n\n#print(total_per_Name)\n# Export the result for control\ntotal_per_Name.to_csv('temp/total_per_Name.csv', index=False)\n\n\n\n\nDrawing a bar graph\nTo improve readability, I display the results of the count using a simple visualization, showing only the top 20 most frequently mentioned characters.\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\nimport matplotlib.font_manager as fm\n# Use SimHei font for Chinese support on Windows\nfont_path = \"C:/Windows/Fonts/simhei.ttf\"\nzh_font = fm.FontProperties(fname=font_path)\n\n\n# Select top 20\ntop_20 = total_per_Name.head(20)\n\n# Plot\nplt.figure(figsize=(12, 8))\n\n# bar management\nbars = plt.barh(top_20[\"Name\"], top_20[\"Count\"], color=\"lightpink\")\n# Add count values to the right of each bar\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width + 1, bar.get_y() + bar.get_height() / 2,\n             str(width), va='center', fontsize=10, fontproperties=zh_font)\n\nplt.barh(top_20[\"Name\"], top_20[\"Count\"], color=\"lightpink\")\nplt.xlabel(\"Count\", fontproperties=zh_font)\nplt.ylabel(\"Name\", fontproperties=zh_font)\nplt.title(\"Top 20 Most Mentioned Names\")\nplt.gca().invert_yaxis()  # So the highest is at the top\n#plt.tight_layout()\nplt.yticks(fontproperties=zh_font)\nplt.xticks(fontproperties=zh_font)\nplt.show()\n\n\n\n\n\nMost mentioned characters"
  },
  {
    "objectID": "index.html#a-network-graph-of-all-the-relations-between-20-most-mentioned-characters",
    "href": "index.html#a-network-graph-of-all-the-relations-between-20-most-mentioned-characters",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "A network graph of all the relations between 20 most mentioned characters",
    "text": "A network graph of all the relations between 20 most mentioned characters\nThat’s ambitious. I will count how many times a pair of characters (story characters) are mentioned together. I choose a threshold of 20 characters (written characters) between two mentioned names.\n\nRetrieving the match positions\nExpanding userdict_simplified.json to add the Chapter_num where the NamePattern is mentioned and the numerical position of each match.\n\n\n\nName\nNamePattern\nChapter_num\nPosition in RawContent\n\n\n\n\n贾宝玉\n贾宝玉\n1\n3650\n\n\n贾宝玉\n宝玉\n6\n5896\n\n\n…\n…\n\n\n\n\n\n\n\nCode\nresults = []\n\n# Iterate over each row in df\nfor i, row in df_cn.iterrows():\n    chapter_num = row['Chapter_num']\n    raw_content = row['RawContent']\n\n    # Check each keyword pattern against this content\n    for _, kw_row in df_keywords.iterrows():\n        name = kw_row['Name']\n        pattern = kw_row['NamePattern']\n\n        # Find all matches of the pattern in the content\n        for match in re.finditer(re.escape(pattern), raw_content):\n            results.append({\n                'Name': name,\n                'NamePattern': pattern,\n                'Chapter_num': chapter_num,\n                'Position': match.start()\n            })\n\n# Convert the results into a new DataFrame\ndf_matches = pd.DataFrame(results)\n\n#print(df_matches)\n\n# Export for control\ndf_matches.to_csv('temp/count_char_position.csv', index=False)\n\n\n\nCounting each Name for control\n\n\nCode\nname_counts = df_matches.groupby('Name').size().reset_index(name='Count')\nname_counts = name_counts.sort_values(by='Count', ascending=False)\n\n#print(name_counts)\n#print(total_per_Name)\n\n\nChecked ✅\n\n\n\nBuilding the matrix\nNow i want a matrix of NamePattern that count characters presents in the same chapter.\n\n\n\n\nNamePattern1\nNamePattern2\nNamePattern3\n…\n\n\n\n\nNamePattern1\n4\n0\n2\n\n\n\nNamePattern2\n0\n3\n5\n\n\n\nNamePattern3\n2\n5\n3\n\n\n\n…\n\n\n\n\n\n\n\nThe following code took 12 hours (!) to complete on my laptop. ⬇️\n\n\nCode\n\"\"\" UNCOMMENT TO REACTIVATE \n\nimport pandas as pd\nimport numpy as np\n\n# Get unique NamePatterns\nname_patterns = df_matches['NamePattern'].unique()\n\n# Initialize the matrix with zeros\nmatrix = pd.DataFrame(0, index=name_patterns, columns=name_patterns)\n\n# Iterate through each pair of matches\nfor i, row_i in df_matches.iterrows():\n    for j, row_j in df_matches.iterrows():\n        if i &gt;= j:\n            continue  # Avoid duplicate and self comparisons\n\n        # Check if in same chapter\n        if row_i['Chapter_num'] == row_j['Chapter_num']:\n            # Check if within +/- 20 positions\n            if abs(row_i['Position'] - row_j['Position']) &lt;= 20:\n                matrix.loc[row_i['NamePattern'], row_j['NamePattern']] += 1\n                matrix.loc[row_j['NamePattern'], row_i['NamePattern']] += 1  # symmetric\n                \n# Export for control\nmatrix.to_csv('temp/matrix_proximity.csv', index=True)\n\n\"\"\"\n\n\n\" UNCOMMENT TO REACTIVATE \\n\\nimport pandas as pd\\nimport numpy as np\\n\\n# Get unique NamePatterns\\nname_patterns = df_matches['NamePattern'].unique()\\n\\n# Initialize the matrix with zeros\\nmatrix = pd.DataFrame(0, index=name_patterns, columns=name_patterns)\\n\\n# Iterate through each pair of matches\\nfor i, row_i in df_matches.iterrows():\\n    for j, row_j in df_matches.iterrows():\\n        if i &gt;= j:\\n            continue  # Avoid duplicate and self comparisons\\n\\n        # Check if in same chapter\\n        if row_i['Chapter_num'] == row_j['Chapter_num']:\\n            # Check if within +/- 20 positions\\n            if abs(row_i['Position'] - row_j['Position']) &lt;= 20:\\n                matrix.loc[row_i['NamePattern'], row_j['NamePattern']] += 1\\n                matrix.loc[row_j['NamePattern'], row_i['NamePattern']] += 1  # symmetric\\n                \\n# Export for control\\nmatrix.to_csv('temp/matrix_proximity.csv', index=True)\\n\\n\"\n\n\nReading the csv previously generated (to avoid recalculating)\n\n\nCode\nimport pandas as pd\n\n# Load the matrix from CSV\nmatrix = pd.read_csv('temp/matrix_proximity.csv', index_col=0)\n\n\nFinalizing counting of proximity by merging the NamePattern (adding their values) depending on their respective Name.\n\n\nCode\n# Replace NamePattern by the associated Name\n\n# Create mapping from NamePattern to Name\nname_map = df_matches.drop_duplicates('NamePattern').set_index('NamePattern')['Name']\n\nmatrix_renamed = matrix\n\n# Rename rows and columns\nmatrix_renamed.rename(index=name_map, columns=name_map, inplace=True)\n\n# Merge all identical Name rows and columns to aggregate the previous synonimous in NamePattern\n\n# Group and sum rows with the same name\nmatrix_renamed = matrix_renamed.groupby(level=0).sum()\n\n# Group and sum columns with the same name (use transpose instead of axis=1)\nmatrix_renamed = matrix_renamed.T.groupby(level=0).sum().T\n\n# Give a value of 0 for the intersection of column = row\nnp.fill_diagonal(matrix_renamed.values, 0)\n\n# Export for control\nmatrix_renamed.to_csv('temp/matrix_renamed_proximity.csv', index=True)\n\n\nKeeping only the highest 20 most proximities\n\n\nCode\n# Compute sum for each row/column\nrow_sums = matrix_renamed.sum(axis=1)\ncol_sums = matrix_renamed.sum(axis=0)\n\n# Get top 20 names by total connections\ntop_names = row_sums.add(col_sums, fill_value=0).sort_values(ascending=False).head(20).index\n\n# Filter matrix to top 20 rows and columns\nmatrix_renamed20 = matrix_renamed.loc[top_names, top_names]\n\n# Export for control\nmatrix_renamed20.to_csv('temp/matrix_renamed20_proximity.csv', index=True)\n\n\n\n\nDrawing a heatmap\nIt represents the proximity between characters throughout the entire novel. We observe that Jia Baoyu interacts more frequently with Lin Daiyu than with Xue Baochai. He also has numerous interactions with Grandmother Jia, Lady Wang, and Jia Xiren.\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\n\n# Use SimHei font for Chinese support on Windows\nfont_path = \"C:/Windows/Fonts/simhei.ttf\"\nzh_font = fm.FontProperties(fname=font_path)\n\n# Set up the figure\nplt.figure(figsize=(12, 10))\n\n# Draw the heatmap\nsns.heatmap(matrix_renamed20, cmap='Reds', linewidths=0.5, square=True, annot=True, fmt='d')\n\n# Add title and axis labels\nplt.title(\"Co-occurrence Heatmap (Top 20 Names)\", fontsize=16, fontproperties=zh_font)\nplt.xlabel(\"Name\", ha='right', fontproperties=zh_font)\nplt.ylabel(\"Name\", fontproperties=zh_font)\n\n# Set tick labels with Chinese font\nplt.xticks(rotation=45, ha='right', fontproperties=zh_font)\nplt.yticks(rotation=0, fontproperties=zh_font)\n\n#plt.tight_layout()\nplt.show()\n\n\n\n\n\nHeatmap of the characters proximity"
  },
  {
    "objectID": "index.html#inspirations-and-further-readings",
    "href": "index.html#inspirations-and-further-readings",
    "title": "Dream of the Red Chamber Data visualizations",
    "section": "Inspirations and further readings",
    "text": "Inspirations and further readings\nBrown, J. Text Analysis and Data Visualization Assignment: Little Women. JBrown’s Blog (2010) https://jbrownsblog.wordpress.com/2010/11/29/text-analysis-and-data-visualization-exercise/\nHeiss, A. PMAP 8921: Data Visualization (2020) https://datavizm20.classes.andrewheiss.com/example/13-example/, updated for 2025 https://datavizsp25.classes.andrewheiss.com/example/14-example.html\nWang, Z., Huang, D., Cui, J. et al. A review of Chinese sentiment analysis: subjects, methods, and trends. Artif Intell Rev 58, 75 (2025). https://doi.org/10.1007/s10462-024-10988-9"
  }
]